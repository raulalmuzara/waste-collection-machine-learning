{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a10bdcb",
   "metadata": {},
   "source": [
    "# CODE - Bachelor's Thesis - Bachelor of Science in Physics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a6bb0e",
   "metadata": {},
   "source": [
    "## Raúl Almuzara\n",
    "\n",
    "### July 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f1e8fc",
   "metadata": {},
   "source": [
    "#### Complete Python code used in the project *Machine learning model to quantify the accordance of work output and objectives in a complex business environment* which is available in English in the PDF file next to this notebook. The original project was written in Spanish and then translated, which is the reason for the name of some variables that have been left untouched.\n",
    "\n",
    "#### This is an end-to-end data science project carried out with Python and QGIS. I present a complete analysis of data provided by a real company related to the management of urban waste collection. The objective is the creation of a goodness metric to determine the quality of the work orders executed by waste collection vehicles.\n",
    "\n",
    "#### First, I carry out an adequate selection and transformation of the data, as well as a study of the variables of greatest interest that can be constructed. Next, I proceed with the design of machine learning models that allow us to make predictions using classification and regression algorithms based on decision trees and neural networks. For each model, we evaluate its capacity with the relevant statistical techniques and I show the most important results for the understanding of the dataset, as well as the implications of the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d6faaa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeda2154",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3f7157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import traj_dist.distance as tdist\n",
    "import random\n",
    "import seaborn as sn\n",
    "from sklearn import tree\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split, validation_curve, KFold, GridSearchCV, cross_validate\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.utils import resample\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "from xgboost import XGBClassifier, XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34a0308",
   "metadata": {},
   "source": [
    "## The following function takes the names of a CSV of theoretical routes, a CSV of actual tracks and a CSV of dumpsters. It creates other 3 CSVs with only those work orders which are common to the three original files, as well as removing duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82d97c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtra_ordenes(csvteoricas,csvreales,csvcoordenadas):\n",
    "    dfteoricas=pd.read_csv(csvteoricas,sep=\";\")\n",
    "    dfreales=pd.read_csv(csvreales,sep=\";\")\n",
    "    dfcoordenadas=pd.read_csv(csvcoordenadas,sep=\";\")\n",
    "    \n",
    "    dfcoordenadas[\"Latitude\"] = dfcoordenadas[\"Latitude\"].astype(str).str.replace(\",\",\".\")\n",
    "    dfcoordenadas[\"Longitude\"] = dfcoordenadas[\"Longitude\"].astype(str).str.replace(\",\",\".\")\n",
    "    \n",
    "    todasreales = pd.Series(dfreales['WorkOrderId'])\n",
    "    duplicadas = list(todasreales[todasreales.duplicated()])    \n",
    "    \n",
    "    ordenesteoricas=np.unique(dfteoricas['WorkOrderId'])\n",
    "    ordenesreales = np.array([x for x in dfreales['WorkOrderId'] if x not in duplicadas])\n",
    "    ordenescoordenadas=np.unique(dfcoordenadas['WorkOrderId'])\n",
    "    ordenes=[ordenesteoricas,ordenesreales,ordenescoordenadas]\n",
    "    ordenescomunes=np.sort(list(set.intersection(*map(set,ordenes))))\n",
    "    \n",
    "    filtroteorico=dfteoricas[dfteoricas['WorkOrderId'].isin(ordenescomunes)]\n",
    "    filtroreal=dfreales[dfreales['WorkOrderId'].isin(ordenescomunes)]\n",
    "    filtrocoord=dfcoordenadas[dfcoordenadas['WorkOrderId'].isin(ordenescomunes)]\n",
    "    \n",
    "    filtrocoordfinal = pd.DataFrame(columns=filtrocoord.columns)\n",
    "    \n",
    "    for orden in ordenescomunes:\n",
    "        filtrocoordfinal = filtrocoordfinal.append(filtrocoord[filtrocoord['WorkOrderId']==orden].drop_duplicates(subset=['ContainerId']),ignore_index=True)\n",
    "        \n",
    "    filtroteorico.to_csv('FiltroTeorico_'+csvteoricas,sep=';',index=False)\n",
    "    filtroreal.to_csv('FiltroReal_'+csvreales,sep=';',index=False)\n",
    "    filtrocoordfinal.to_csv('FiltroCoord_'+csvcoordenadas,sep=';',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cba522",
   "metadata": {},
   "source": [
    "## Densification function (it adds extra equispaced points to the arrays of points that define the trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065d2b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def densificar(coordenadas, segmentos):\n",
    "    denso=coordenadas\n",
    "    for i in range(len(coordenadas)-1):\n",
    "        denso=np.insert(denso,i*segmentos+1,np.array([list(a) for a in zip(np.linspace(coordenadas[i][0],coordenadas[i+1][0],segmentos,endpoint=False)[1:],np.linspace(coordenadas[i][1],coordenadas[i+1][1],segmentos,endpoint=False)[1:])]),0)\n",
    "    return denso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f441ef91",
   "metadata": {},
   "source": [
    "## Definition of the function that processes the data to obtain relevant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e002cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Secondary function that helps the main function\n",
    "def clasifica1(cen_x):\n",
    "    if cen_x==centroides[0]:\n",
    "        return 0\n",
    "    if cen_x==centroides[1]:\n",
    "        return 1\n",
    "    if cen_x==centroides[2]:\n",
    "        return 2\n",
    "    if cen_x==centroides[3]:\n",
    "        return 3\n",
    "\n",
    "#Secondary function that helps the main function\n",
    "def clasifica2(Bondad):\n",
    "    if Bondad>=0 and Bondad<min1:\n",
    "        return 0\n",
    "    if Bondad>=min1 and Bondad<min2:\n",
    "        return 1\n",
    "    if Bondad>=min2 and Bondad<min3:\n",
    "        return 2\n",
    "    if Bondad>=min3:\n",
    "        return 3\n",
    "\n",
    "#Main function that generates all variables.\n",
    "#It takes 3 CSVs of theoretical routes, actual routes and filtered containers (by the filtering function above).\n",
    "#It returns a dataframe.\n",
    "def calcula_variables(csvteoricas,csvreales,csvcoordenadas):\n",
    "\n",
    "    ordenes = pd.read_csv(csvcoordenadas,sep=';')\n",
    "\n",
    "    ordenesag = ordenes.groupby('WorkOrderId').agg({\n",
    "        'Scheduled': 'sum',\n",
    "        'Collected': 'sum'\n",
    "    })\n",
    "    \n",
    "    #Obtains number of containers of each type in each work order\n",
    "    ordenesag['NContenedores']=ordenes.groupby('WorkOrderId').count().iloc[:,2]\n",
    "    ordenesag['01']=ordenesag['NContenedores']-ordenesag['Scheduled']\n",
    "    ordenesag['10']=ordenesag['NContenedores']-ordenesag['Collected']\n",
    "    ordenesag['11']=ordenesag['NContenedores']-ordenesag['01']-ordenesag['10']\n",
    "    ordenesag=ordenesag.drop(columns=['Scheduled','Collected'])\n",
    "    ordenesag['Ruido'] = None #Column for random noise\n",
    "    ordenesag['BienHechos']=ordenesag['11']/(ordenesag['11']+ordenesag['10']) #Proportion of well-collected containers\n",
    "    ordenesag['Similitud'] = None #Degree of similarity\n",
    "    ordenesag['Adicionales']=ordenesag['01']/(ordenesag['11']+ordenesag['10']+ordenesag['01']) #Proportion of additional containers collected\n",
    "    ordenesag['RatioLongitudes'] = None #Relative deviation between theoretical and actual length\n",
    "\n",
    "    teoricas = pd.read_csv(csvteoricas,sep=\";\")\n",
    "    reales = pd.read_csv(csvreales,sep=\";\")\n",
    "\n",
    "    for j in range(len(reales['WorkOrderId'])):\n",
    "        wktr = reales['WKT'].iloc[j].replace(\"LINESTRING(\" , \"\").replace(\")\",\"\").replace(\",\",\" \").split()\n",
    "        wktreales = []\n",
    "        \n",
    "        #Obtains the array of points of the actual route and its length\n",
    "        for i in range(0,len(wktr),2):\n",
    "            wktreales.append([float(wktr[i]),float(wktr[i+1])])\n",
    "        longitudes_r = np.sqrt(np.sum(np.diff(np.array(wktreales), axis=0)**2, axis=1))\n",
    "        longitud_real = np.sum(longitudes_r)\n",
    "\n",
    "        wktt = teoricas['WKT'].iloc[j].replace(\"MULTILINESTRING((\" , \"\").replace(\"))\",\"\").replace(\",\",\" \").split()\n",
    "        wktteoricas = []\n",
    "        \n",
    "        #Obtains the array of points of the theoretical route and its length\n",
    "        for i in range(0,len(wktt),2):\n",
    "            wktteoricas.append([float(wktt[i]),float(wktt[i+1])])\n",
    "        longitudes_t = np.sqrt(np.sum(np.diff(np.array(wktteoricas), axis=0)**2, axis=1))\n",
    "        longitud_teorica = np.sum(longitudes_t)\n",
    "\n",
    "        ratio = np.abs(longitud_real-longitud_teorica)/longitud_teorica\n",
    "        ordenesag['RatioLongitudes'].iloc[j]=ratio\n",
    "\n",
    "        distancia = tdist.sspd(np.array(wktreales),np.array(wktteoricas),'euclidean') #For higher accuracy, use 'spherical' (haversine formula) if the points are pairs (longitude,latitude)\n",
    "        ordenesag['Similitud'].iloc[j]=distancia\n",
    "        \n",
    "        ordenesag['Ruido'].iloc[j]=np.random.normal(0,0.7) #To make the models more realistic and hinder training, increase the standard deviation of the Gaussian noise\n",
    "    \n",
    "    ordenesag['Similitud']=0.001/(0.001+ordenesag['Similitud']) #Normalization in the range [0,1]. Parameter must be carefully chosen (0.001)\n",
    "    \n",
    "    ordenesag['RatioLongitudes']=(ordenesag['RatioLongitudes']-ordenesag['RatioLongitudes'].min())/(ordenesag['RatioLongitudes'].max()-ordenesag['RatioLongitudes'].min()) #Normalization to range [0,1]\n",
    "        \n",
    "    ordenesag['Bondad']=10*ordenesag['BienHechos']+4*ordenesag['Similitud']+2*ordenesag['Adicionales']-1*ordenesag['RatioLongitudes']+ordenesag['Ruido'] #Linear combination with weights 10, 4, 2, -1 plus noise\n",
    "    \n",
    "    ordenesag['Bondad']=10*(ordenesag['Bondad']-ordenesag['Bondad'].min())/(ordenesag['Bondad'].max()-ordenesag['Bondad'].min()) #Normalization to range [0,10]\n",
    "    \n",
    "    \n",
    "    #Clustering\n",
    "    \n",
    "    mezcla = ordenesag.copy()\n",
    "\n",
    "    kmeans = KMeans(n_clusters=4, random_state=0)\n",
    "    mezcla['cluster'] = kmeans.fit_predict(mezcla[['Bondad']])\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    cen_x = [i[0] for i in centroids] \n",
    "    mezcla['cen_x'] = mezcla.cluster.map({0:cen_x[0], 1:cen_x[1], 2:cen_x[2], 3:cen_x[3]})\n",
    "    \n",
    "    global centroides\n",
    "    \n",
    "    centroides = np.sort(np.unique(mezcla['cen_x']))\n",
    "    colors = ['#DF2020', '#81DF20', '#2095DF', '#20DF95']\n",
    "    mezcla['c'] = mezcla.cluster.map({0:colors[0], 1:colors[1], 2:colors[2], 3:colors[3]})\n",
    "    plt.scatter(mezcla.Bondad,mezcla.Bondad, c=mezcla.c, alpha = 0.6, s=10)\n",
    "    ordenesag['Resultado']=mezcla['cen_x'].apply(clasifica1)\n",
    "    \n",
    "    global min0\n",
    "    global min1\n",
    "    global min2\n",
    "    global min3\n",
    "    \n",
    "    min0 = min(ordenesag[ordenesag['Resultado']==0]['Bondad'])\n",
    "    min1 = min(ordenesag[ordenesag['Resultado']==1]['Bondad'])\n",
    "    min2 = min(ordenesag[ordenesag['Resultado']==2]['Bondad'])\n",
    "    min3 = min(ordenesag[ordenesag['Resultado']==3]['Bondad'])\n",
    "    \n",
    "    print(min0)\n",
    "    print(min1)\n",
    "    print(min2)\n",
    "    print(min3)\n",
    "    \n",
    "    ordenesag['Bondad']=ordenesag['Bondad'].astype(float).round(0)\n",
    "    \n",
    "    ordenesag['Resultado']=ordenesag['Bondad'].apply(clasifica2)\n",
    "    \n",
    "    return ordenesag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233ec0cc",
   "metadata": {},
   "source": [
    "## Calculate variables (note that each run will give somewhat different results due to random noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3fcfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = calcula_variables('Rutas_teoricas_623_OTs.csv','Rutas_reales_623_OTs.csv','Contenedores_623_OTs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e303c4",
   "metadata": {},
   "source": [
    "## Matrix of plots with variables taken in pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aa37fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.pairplot(variables[[\"BienHechos\", \"Similitud\", \"Adicionales\", \"RatioLongitudes\"]].astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458143f3",
   "metadata": {},
   "source": [
    "## Decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f74025",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = variables[['BienHechos','Similitud','Adicionales','RatioLongitudes']]\n",
    "y = variables['Resultado']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=3,random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "fig = plt.figure(figsize=(100,80))\n",
    "tree.plot_tree(clf, feature_names=['BienHechos','Similitud','Adicionales','RatioLongitudes'], class_names=['0','1','2','3'],filled=True)\n",
    "plt.show()\n",
    "y_pred=clf.predict(X_test)\n",
    "acc = accuracy_score(y_test,y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201da407",
   "metadata": {},
   "source": [
    "## Confusion matrix for the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de685a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(4,3))\n",
    "sn.heatmap(confusion_matrix(y_test,y_pred),annot=True)\n",
    "plt.title('Matriz de confusión del árbol')\n",
    "plt.xlabel('Clase predicha')\n",
    "plt.ylabel('Clase real')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08632cf",
   "metadata": {},
   "source": [
    "## Accuracy versus number of trees in the random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe97afd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for k in range(1, 100):\n",
    "    rfc = RandomForestClassifier(n_estimators=k,max_depth=3,n_jobs=-1,random_state=42)\n",
    "    rfc.fit(X_train, y_train)\n",
    "    y_pred = rfc.predict(X_test)\n",
    "    scores.append(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(range(1, 100), scores)\n",
    "plt.title('Precisión del bosque aleatorio en función del número de árboles')\n",
    "plt.xlabel('Número de árboles')\n",
    "plt.ylabel('Precisión')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b593342f",
   "metadata": {},
   "source": [
    "## Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce99bc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = variables[['BienHechos','Similitud','Adicionales','RatioLongitudes']]\n",
    "y = variables['Resultado']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=70,max_depth=3,n_jobs=-1,random_state=42)\n",
    "rf.fit(X_train,y_train)\n",
    "y_pred=rf.predict(X_test)\n",
    "acc = accuracy_score(y_test,y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddd80a2",
   "metadata": {},
   "source": [
    "## Confusion matrix for the random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9036aba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(4,3))\n",
    "sn.heatmap(confusion_matrix(y_test,y_pred),annot=True)\n",
    "plt.title('Matriz de confusión del bosque')\n",
    "plt.xlabel('Clase predicha')\n",
    "plt.ylabel('Clase real')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda66df8",
   "metadata": {},
   "source": [
    "## Decision surfaces training random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f13221",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 4\n",
    "plot_colors = ['darkorange','lime','deepskyblue','fuchsia']\n",
    "plot_step = 0.01\n",
    "\n",
    "for pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3],\n",
    "                                [1, 2], [1, 3], [2, 3]]):\n",
    "    \n",
    "    X = np.array(variables[['BienHechos','Similitud','Adicionales','RatioLongitudes']])[:, pair]\n",
    "    y = np.array(variables['Resultado'])\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=70,max_depth=3,n_jobs=-1,random_state=42).fit(X, y)\n",
    "\n",
    "    plt.subplot(2, 3, pairidx + 1)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.gist_rainbow)\n",
    "\n",
    "    plt.xlabel(['BienHechos','Similitud','Adicionales','RatioLongitudes'][pair[0]])\n",
    "    plt.ylabel(['BienHechos','Similitud','Adicionales','RatioLongitudes'][pair[1]])\n",
    "\n",
    "    for i, color in zip(range(n_classes), plot_colors):\n",
    "        idx = np.where(y == i)\n",
    "        plt.scatter(X[idx, 0], X[idx, 1], c=color, label=['0','1','2','3'][i],\n",
    "                    cmap=plt.cm.rainbow, edgecolor='black', s=10)\n",
    "\n",
    "plt.suptitle(\"Superficies de decisión\")\n",
    "plt.legend(loc='lower right', borderpad=0, handletextpad=0)\n",
    "plt.axis(\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a510ca0",
   "metadata": {},
   "source": [
    "## Feature importances training a random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf98a004",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = rf.feature_importances_\n",
    "\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "\n",
    "plt.bar([x for x in range(len(importance))], importance)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87765f45",
   "metadata": {},
   "source": [
    "## Applying bootstrapping to a random forest to obtain a 95% confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8a7e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = variables[['BienHechos','Similitud','Adicionales','RatioLongitudes','Resultado']].astype(float).values\n",
    "\n",
    "n_iterations = 1000\n",
    "n_size = int(len(variables) * 1)\n",
    "\n",
    "stats = list()\n",
    "for i in range(n_iterations):\n",
    "\n",
    "    train = resample(values, n_samples=n_size)\n",
    "    test = np.array([x for x in values if x.tolist() not in train.tolist()])\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=70,max_depth=3,n_jobs=-1,random_state=42)\n",
    "    model.fit(train[:,:-1], train[:,-1])\n",
    "\n",
    "    predictions = model.predict(test[:,:-1])\n",
    "    score = accuracy_score(test[:,-1], predictions)\n",
    "    print(score)\n",
    "    stats.append(score)\n",
    "\n",
    "plt.hist(stats)\n",
    "plt.show()\n",
    "\n",
    "alpha = 0.95\n",
    "p = ((1.0-alpha)/2.0) * 100\n",
    "lower = max(0.0, np.percentile(stats, p))\n",
    "p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "upper = min(1.0, np.percentile(stats, p))\n",
    "print('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71e585e",
   "metadata": {},
   "source": [
    "## Multiclass ROC curves for the random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea01c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(variables[['BienHechos','Similitud','Adicionales','RatioLongitudes']])\n",
    "y = np.array(variables['Resultado'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=70,max_depth=3,n_jobs=-1,random_state=42)\n",
    "visualizer = ROCAUC(model, micro=False, macro=False, classes=[\"0\", \"1\", \"2\", \"3\"])\n",
    "\n",
    "visualizer.fit(X_train, y_train)\n",
    "visualizer.score(X_test, y_test)\n",
    "\n",
    "custom_viz = visualizer.ax\n",
    "custom_viz.set_title(\"Curvas ROC para bosque aleatorio\")\n",
    "custom_viz.set_xlabel(\"1 - Especificidad\")\n",
    "custom_viz.set_ylabel(\"Sensibilidad\")\n",
    "custom_viz.legend()\n",
    "custom_viz.figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2928b1b2",
   "metadata": {},
   "source": [
    "## Random forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a15275",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = variables[['BienHechos','Similitud','Adicionales','RatioLongitudes']]\n",
    "y = variables['Bondad']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=70,max_depth=3,random_state=42)\n",
    "rf.fit(X_train,y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "rf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6379df49",
   "metadata": {},
   "source": [
    "## Tuning a multilayer perceptron classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73a6e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = variables[['BienHechos','Similitud','Adicionales','RatioLongitudes']]\n",
    "y = variables['Resultado']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "mlp = MLPClassifier()\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(4,),(5,),(6,)],\n",
    "    'activation': ['tanh','logistic','relu'],\n",
    "    'solver': ['lbfgs'],\n",
    "    'alpha': [0.01,0.001,0.0001,0.00001],\n",
    "    'max_iter': [1000],\n",
    "}\n",
    "clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print('Mejores parametros:\\n', clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fe79ba",
   "metadata": {},
   "source": [
    "## Confusion matrix for the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e20fe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(4,),activation='relu',solver='lbfgs',alpha=0.0001,learning_rate='constant',max_iter=2000)\n",
    "mlp.fit(X_train,y_train)\n",
    "print(mlp.score(X_test,y_test))\n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(4,3))\n",
    "sn.heatmap(confusion_matrix(y_test,mlp.predict(X_test)),annot=True)\n",
    "plt.xlabel('Clase predicha')\n",
    "plt.ylabel('Clase real')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f365ea",
   "metadata": {},
   "source": [
    "## Applying bootstrapping to a neural network to obtain a 95% confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3da65fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = variables[['BienHechos','Similitud','Adicionales','RatioLongitudes','Resultado']].astype(float).values\n",
    "\n",
    "n_iterations = 1000\n",
    "n_size = int(len(variables) * 0.5)\n",
    "\n",
    "stats = list()\n",
    "for i in range(n_iterations):\n",
    "\n",
    "    train = resample(values, n_samples=n_size)\n",
    "    test = np.array([x for x in values if x.tolist() not in train.tolist()])\n",
    "\n",
    "    model = MLPClassifier(hidden_layer_sizes=(4,),activation='relu',solver='lbfgs',alpha=0.0001,max_iter=2000,learning_rate='constant')\n",
    "    model.fit(train[:,:-1], train[:,-1])\n",
    "\n",
    "    predictions = model.predict(test[:,:-1])\n",
    "    score = accuracy_score(test[:,-1], predictions)\n",
    "    print(score)\n",
    "    stats.append(score)\n",
    "\n",
    "plt.hist(stats)\n",
    "plt.show()\n",
    "\n",
    "alpha = 0.95\n",
    "p = ((1.0-alpha)/2.0) * 100\n",
    "lower = max(0.0, np.percentile(stats, p))\n",
    "p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "upper = min(1.0, np.percentile(stats, p))\n",
    "print('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))\n",
    "\n",
    "variables[['BienHechos','Similitud','Adicionales','RatioLongitudes','Resultado']].astype(float).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79002d4b",
   "metadata": {},
   "source": [
    "## Training a neural network with Adam in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6967ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x_data, y_data, batch_size):\n",
    "    idxs = np.random.randint(0, len(y_data), batch_size)\n",
    "    return x_data[idxs,:], y_data[idxs]\n",
    "\n",
    "def nn_model(x_input, W1, b1, W2, b2):\n",
    "    x_input = tf.reshape(x_input, (x_input.shape[0], -1))\n",
    "    x = tf.add(tf.matmul(tf.cast(x_input, tf.float32), W1), b1)\n",
    "    x = tf.nn.relu(x)\n",
    "    logits = tf.add(tf.matmul(x, W2), b2)\n",
    "    return logits\n",
    "\n",
    "def loss_fn(logits, labels):\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels,\n",
    "                                                                              logits=logits))\n",
    "    return cross_entropy\n",
    "\n",
    "epochs = 80\n",
    "batch_size = 50\n",
    "\n",
    "X = variables[['BienHechos','Similitud','Adicionales','RatioLongitudes']]\n",
    "y = variables['Resultado']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train = tf.Variable(np.array(X_train).astype('float'))\n",
    "X_test = tf.Variable(np.array(X_test).astype('float'))\n",
    "\n",
    "W1 = tf.Variable(tf.random.normal([4, 4], stddev=0.03), name='W1')\n",
    "\n",
    "b1 = tf.Variable(tf.random.normal([4]), name='b1')\n",
    "\n",
    "W2 = tf.Variable(tf.random.normal([4, 4], stddev=0.03), name='W2')\n",
    "\n",
    "b2 = tf.Variable(tf.random.normal([4]), name='b2')\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.07,beta_1=0.9,beta_2=0.999,epsilon=0.0000001)\n",
    "\n",
    "total_batch = int(len(y_train) / batch_size)\n",
    "arrayloss=[]\n",
    "arrayacc=[]\n",
    "for epoch in range(epochs):\n",
    "    avg_loss = 0\n",
    "    for i in range(total_batch):\n",
    "        batch_x, batch_y = get_batch(np.array(X_train), np.array(y_train), batch_size=batch_size)\n",
    "\n",
    "        batch_x = tf.Variable(np.array(batch_x).astype('float'))\n",
    "        batch_y = tf.Variable(np.array(batch_y).astype('float'))\n",
    "\n",
    "        batch_y = tf.one_hot(np.array(batch_y).astype('int'),4)\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = nn_model(batch_x, W1, b1, W2, b2)\n",
    "            loss = loss_fn(logits, batch_y)\n",
    "        gradients = tape.gradient(loss, [W1, b1, W2, b2])\n",
    "        optimizer.apply_gradients(zip(gradients, [W1, b1, W2, b2]))\n",
    "        avg_loss += loss / total_batch\n",
    "    arrayloss.append(avg_loss)\n",
    "    test_logits = nn_model(X_test, W1, b1, W2, b2)\n",
    "    max_idxs = tf.argmax(test_logits, axis=1)\n",
    "    test_acc = np.sum(max_idxs.numpy() == y_test) / len(y_test)\n",
    "    arrayacc.append(test_acc)\n",
    "    print(f\"Epoca: {epoch + 1}, perdida={avg_loss:.3f}, precision={test_acc*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5208b5b7",
   "metadata": {},
   "source": [
    "## Loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6366fe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,epochs+1),arrayloss,color='red')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.title('Curva de pérdida')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a69e0c",
   "metadata": {},
   "source": [
    "## Accuracy curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462aaa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,epochs+1),arrayacc,color='green')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Precisión')\n",
    "plt.title('Curva de precisión')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66cf111",
   "metadata": {},
   "source": [
    "## Multiclass ROC curves for the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55e2ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(variables[['BienHechos','Similitud','Adicionales','RatioLongitudes']])\n",
    "y = np.array(variables['Resultado'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "model = MLPClassifier(hidden_layer_sizes=(4,),activation='relu',solver='lbfgs',alpha=0.0001,learning_rate='constant',max_iter=2000)\n",
    "visualizer = ROCAUC(model, micro=False, macro=False, classes=[\"0\", \"1\", \"2\", \"3\"])\n",
    "\n",
    "visualizer.fit(X_train, y_train)\n",
    "visualizer.score(X_test, y_test)\n",
    "\n",
    "custom_viz = visualizer.ax\n",
    "custom_viz.set_title(\"Curvas ROC para la red neuronal\")\n",
    "custom_viz.set_xlabel(\"1 - Especificidad\")\n",
    "custom_viz.set_ylabel(\"Sensibilidad\")\n",
    "custom_viz.legend()\n",
    "custom_viz.figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0b34b6",
   "metadata": {},
   "source": [
    "## Neural network regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da631846",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = variables[['BienHechos','Similitud','Adicionales','RatioLongitudes']]\n",
    "y = variables['Bondad']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "mlp = MLPRegressor()\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(4,)],\n",
    "    'activation': ['relu'],\n",
    "    'solver': ['lbfgs'],\n",
    "    'alpha': [0.0001],\n",
    "    'learning_rate': ['constant'],\n",
    "}\n",
    "regr = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n",
    "regr.fit(X_train, y_train)\n",
    "y_pred = regr.predict(X_test)\n",
    "regr.score(X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
